{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchaudio import transforms\n",
    "\n",
    "from models.audio_LSTMCNN import AudioLSTMCNN\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE = 44100\n",
    "MEL_SPECTROGRAM_BUCKETS = 128\n",
    "MEL_SPECTROGRAM_WINDOW_LENGTH = 224\n",
    "SPECTROS_PER_SECOND = RATE // (MEL_SPECTROGRAM_WINDOW_LENGTH / 2) - 1\n",
    "CHUNKS_PER_SECOND = 2\n",
    "CHUNK_SIZE_IN_SPECTROS = int(SPECTROS_PER_SECOND // CHUNKS_PER_SECOND)\n",
    "\n",
    "TRAINSET_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label = '63161616'\n",
    "with open(f'data/datasets/trainset_{dataset_label}', 'rb') as f:\n",
    "    trainset = pickle.load(f)\n",
    "with open(f'data/datasets/testset_{dataset_label}.pk', 'rb') as f:\n",
    "    trainset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FOLDER = \"C:\\\\Users\\\\amity\\\\PycharmProjects\\\\aimpathy\\\\data\\\\PMEmo\\\\PMEmo2019\\\\chorus\"\n",
    "THAYER_ANOTATIONS_CSV = \"C:\\\\Users\\\\amity\\\\PycharmProjects\\\\aimpathy\\\\data\\\\PMEmo\\\\PMEmo2019\\\\annotations\\\\dynamic_annotations.csv\"\n",
    "thayer_annotations_df = pd.read_csv(THAYER_ANOTATIONS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = [f for f in os.listdir(AUDIO_FOLDER) if isfile(join(AUDIO_FOLDER, f))]\n",
    "audio_data = dict()\n",
    "torch_spectorgrams = dict()\n",
    "spectorgrammer = transforms.MelSpectrogram(sample_rate=RATE, n_fft=(MEL_SPECTROGRAM_BUCKETS * 2 - 2), win_length=MEL_SPECTROGRAM_WINDOW_LENGTH, power=2, normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convery to Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = dict()\n",
    "for audio_file in tqdm(audio_files, total=len(audio_files)):\n",
    "    sound = AudioSegment.from_mp3(os.sep.join([AUDIO_FOLDER, audio_file])).set_channels(1)\n",
    "    audio_file_wave = sound.export(format=\"wav\", bitrate=RATE)\n",
    "    sample_rate, samples = wavfile.read(audio_file_wave)\n",
    "    spectogram = spectorgrammer(torch.from_numpy(samples/(2**15)).float().reshape((1, -1)))\n",
    "    spectrograms[audio_file] = spectogram\n",
    "    audio_file_wave.close()\n",
    "spectrograms = {key: value for key, value in spectrograms.items() if \".wav\" not in key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict()  # music_id: spectrogram, (valence, arousal)\n",
    "for file_name, spectrogram in tqdm(spectrograms.items(), total=len(spectrograms)):\n",
    "    music_id = int(file_name.replace(\".mp3\", \"\"))\n",
    "    dataset[music_id] = list()\n",
    "    max_frame_time = thayer_annotations_df[thayer_annotations_df[\"musicId\"] == music_id][\"frameTime\"].max()\n",
    "    if np.isnan(max_frame_time):\n",
    "        # print(f\"Skipping {music_id}\")\n",
    "        continue\n",
    "    for i in range(int(CHUNKS_PER_SECOND * max_frame_time)):\n",
    "        data_df = thayer_annotations_df[(thayer_annotations_df[\"musicId\"] == music_id) & (thayer_annotations_df[\"frameTime\"] == i/2)]\n",
    "        if data_df.empty:\n",
    "            #  print(f\"Skipping {musicI_id} - {i/2}\")\n",
    "            continue\n",
    "        valence = data_df.iloc[0][\"Valence(mean)\"]\n",
    "        arousal = data_df.iloc[0][\"Arousal(mean)\"]\n",
    "        dataset[music_id].append((spectrogram[0, :, int((i/2-1)*CHUNK_SIZE_IN_SPECTROS): int((i/2)*CHUNK_SIZE_IN_SPECTROS)], (valence, arousal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_music_ids = random.sample(dataset.keys(), int(TRAINSET_RATIO*len(dataset)))\n",
    "trainset = {key: value for key, value in dataset.items() if key in train_music_ids}\n",
    "testset = {key: value for key, value in dataset.items() if key not in train_music_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/datasets/trainset_{int(time.time())}.pk', 'wb') as f:\n",
    "    pickle.dump(trainset, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(f'data/datasets/testset_{int(time.time())}.pk', 'wb') as f:\n",
    "    pickle.dump(testset, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title=None, ylabel=\"freq_bin\", aspect=\"auto\", xmax=None, ymax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(spec), origin=\"lower\", aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    if ymax:\n",
    "        axs.set_ylim((0, ymax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(dataset[1000][0], title=\"MelSpectrogram - torchaudio\", ylabel=\"mel freq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# model_name = f'saved_models/{}.pt'\n",
    "# model_c = AudioLSTMCNN(input_shape=(128, 196), out_size=2).cuda()\n",
    "# model_c.load_state_dict(torch.load(latest_model_name)).cuda();\n",
    "# model_c.eval()\n",
    "# print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = AudioLSTMCNN(input_shape=(128, 196), out_size=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = torch.optim.Adam(model_c.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[10][0][0].shape)\n",
    "print(dataset[10][0][0].reshape((1, 1, MEL_SPECTROGRAM_BUCKETS, -1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "EPOCS = 100\n",
    "PRINT_MARK = 500\n",
    "BATCH_SIZE = 50\n",
    "STOP_LOSS = 0.01\n",
    "MIN_LEARNING_RATE = 0.0001\n",
    "\n",
    "normalize = lambda x: (x + 1)/2\n",
    "denormalize = lambda x: x*2 - 1\n",
    "\n",
    "\n",
    "for epoc in range(EPOCS):\n",
    "    losses = list()\n",
    "    train_key_sample = random.sample(trainset.keys(), BATCH_SIZE)\n",
    "    train_sample = [datum for key, value in trainset.items() if key in train_key_sample for datum in value]\n",
    "    for batch_i, (X_train, (valence, arousal)) in enumerate(train_sample):\n",
    "        optimizer.zero_grad()\n",
    "        model_c.hidden = (torch.zeros(model.hidden[0].shape).cuda(),\n",
    "                        torch.zeros(model.hidden[0].shape).cuda())\n",
    "        \n",
    "        y_train = torch.Tensor((valence, arousal)).cuda()\n",
    "        # Apply the model\n",
    "        y_pred = model_c(X_train.cuda())  # we don't flatten X-train here\n",
    "        loss = criterion(y_pred, y_train)\n",
    "\n",
    "        # Update parameters\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        \n",
    "        # Print interim results\n",
    "        if (batch_i > 0 or epoc == 0) and batch_i%PRINT_MARK == 0:\n",
    "            print(f'{epoc:2}-{batch_i:4} | loss: {loss.item():.5f}[{np.mean(losses):.5f}] [{abs(y_train[0] - y_pred[0]):.5f}, {abs(y_train[1] - y_pred[1]):.5f}]        lr: {optimizer.param_groups[0][\"lr\"]}\\\n",
    "                  {y_train[0].item():.5f}, {y_train[0].item():.5f}  |  {y_pred[0].item():.5f}, {y_pred[0].item():.5f}')\n",
    "    \n",
    "    scheduler.step(np.mean(losses))\n",
    "\n",
    "    if np.mean(losses) < STOP_LOSS or optimizer.param_groups[0][\"lr\"] < MIN_LEARNING_RATE:\n",
    "        break\n",
    "        \n",
    "latest_model_name = f'saved_models/AudioLSTMCNN_{int(time.time())}_{str(np.mean(losses))[2:]}.pt'\n",
    "torch.save(model_c.state_dict(), latest_model_name)\n",
    "print(f\"Saved model {latest_model_name}\")\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_sample = [datum for key, value in testset.items() for datum in value]\n",
    "    for batch_i, (X_test, (valence, arousal)) in tqdm(enumerate(test_sample), total=len(test_sample)):\n",
    "        model_c.hidden = (torch.zeros(model.hidden[0].shape).cuda(),\n",
    "                        torch.zeros(model.hidden[0].shape).cuda())\n",
    "        y_test = torch.Tensor((valence, arousal)).cuda()\n",
    "        # Apply the model\n",
    "        y_val = model_c(X_test.cuda())\n",
    "        loss = criterion(y_val, y_test)\n",
    "        losses.append(loss.cpu())\n",
    "        if (min(y_val).item() < 0.5):\n",
    "            print(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {np.mean(losses):.5f}, median: {np.median(losses):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect quadrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 519\n",
    "test_location = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model_c(testset[test_index][test_location][0].cuda()).cpu()\n",
    "print(pred)\n",
    "print([denormalize(a.item()) for a in pred])\n",
    "print(testset[test_index][test_location][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(key, value[0][1]) for key, value in testset.items() if value and value[0][1] < (0.5, 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimpathy",
   "language": "python",
   "name": "aimpathy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

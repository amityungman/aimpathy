{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchaudio import transforms\n",
    "\n",
    "from models.audio_LSTMCNN import AudioLSTMCNN\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE = 44100\n",
    "MEL_SPECTROGRAM_BUCKETS = 128\n",
    "MEL_SPECTROGRAM_WINDOW_LENGTH = 224\n",
    "SPECTROS_PER_SECOND = RATE // (MEL_SPECTROGRAM_WINDOW_LENGTH / 2) - 1\n",
    "CHUNKS_PER_SECOND = 2\n",
    "CHUNK_SIZE_IN_SPECTROS = int(SPECTROS_PER_SECOND // CHUNKS_PER_SECOND)\n",
    "\n",
    "TRAINSET_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quadrant(val: float, aro: float) -> int:\n",
    "    if val >= 0.5 and aro >= 0.5:\n",
    "        return 1\n",
    "    if val >= 0.5 and aro < 0.5:\n",
    "        return 2\n",
    "    if val < 0.5 and aro < 0.5:\n",
    "        return 3\n",
    "    if val < 0.5 and aro >= 0.5:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FOLDER = \"C:\\\\Users\\\\amity\\\\PycharmProjects\\\\aimpathy\\\\data\\\\PMEmo\\\\PMEmo2019\\\\chorus\"\n",
    "THAYER_ANOTATIONS_CSV = \"C:\\\\Users\\\\amity\\\\PycharmProjects\\\\aimpathy\\\\data\\\\PMEmo\\\\PMEmo2019\\\\annotations\\\\dynamic_annotations.csv\"\n",
    "thayer_annotations_df = pd.read_csv(THAYER_ANOTATIONS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amity\\Anaconda3\\envs\\aimpathy\\lib\\site-packages\\torchaudio\\functional\\functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (128) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "audio_files = [f for f in os.listdir(AUDIO_FOLDER) if isfile(join(AUDIO_FOLDER, f))]\n",
    "audio_data = dict()\n",
    "torch_spectorgrams = dict()\n",
    "spectorgrammer = transforms.MelSpectrogram(sample_rate=RATE, n_fft=(MEL_SPECTROGRAM_BUCKETS * 2 - 2), win_length=MEL_SPECTROGRAM_WINDOW_LENGTH, power=2, normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 794/794 [03:17<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "spectrograms = dict()\n",
    "for audio_file in tqdm(audio_files, total=len(audio_files)):\n",
    "    sound = AudioSegment.from_mp3(os.sep.join([AUDIO_FOLDER, audio_file])).set_channels(1)\n",
    "    audio_file_wave = sound.export(format=\"wav\", bitrate=RATE)\n",
    "    sample_rate, samples = wavfile.read(audio_file_wave)\n",
    "    spectogram = spectorgrammer(torch.from_numpy(samples/(2**15)).float().reshape((1, -1)))\n",
    "    spectrograms[audio_file] = spectogram\n",
    "    audio_file_wave.close()\n",
    "spectrograms = {key: value for key, value in spectrograms.items() if \".wav\" not in key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 793/793 [00:31<00:00, 24.93it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dict()  # music_id: spectrogram, (valence, arousal)\n",
    "for file_name, spectrogram in tqdm(spectrograms.items(), total=len(spectrograms)):\n",
    "    music_id = int(file_name.replace(\".mp3\", \"\"))\n",
    "    dataset[music_id] = list()\n",
    "    max_frame_time = thayer_annotations_df[thayer_annotations_df[\"musicId\"] == music_id][\"frameTime\"].max()\n",
    "    if np.isnan(max_frame_time):    \n",
    "        dataset.pop(music_id)\n",
    "        continue\n",
    "    for i in range(int(CHUNKS_PER_SECOND * max_frame_time)):\n",
    "        data_df = thayer_annotations_df[(thayer_annotations_df[\"musicId\"] == music_id) & (thayer_annotations_df[\"frameTime\"] == i/2)]\n",
    "        if data_df.empty:\n",
    "            #  print(f\"Skipping {musicI_id} - {i/2}\")\n",
    "            continue\n",
    "        valence = data_df.iloc[0][\"Valence(mean)\"]\n",
    "        arousal = data_df.iloc[0][\"Arousal(mean)\"]\n",
    "        dataset[music_id].append((spectrogram[0, :, int((i/2-1)*CHUNK_SIZE_IN_SPECTROS): int((i/2)*CHUNK_SIZE_IN_SPECTROS)], (valence, arousal)))\n",
    "    \n",
    "    if len(dataset[music_id]) <= 0:\n",
    "        dataset.pop(music_id)\n",
    "        \n",
    "music_id_to_quadrant = {key: get_quadrant(np.mean([val[1][0] for val in value]), np.mean([val[1][1] for val in value])) for key, value in dataset.items() if value}\n",
    "quadrant_counters = [list(music_id_to_quadrant.values()).count(i+1) for i in range(4)]\n",
    "quadrant_ratios = max(quadrant_counters)//np.array(quadrant_counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_music_ids = random.sample(dataset.keys(), int(TRAINSET_RATIO*len(dataset)))\n",
    "trainset = {key: value for key, value in dataset.items() if key in train_music_ids}\n",
    "testset = {key: value for key, value in dataset.items() if key not in train_music_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/datasets/trainset_{int(time.time())}.pk', 'wb') as f:\n",
    "    pickle.dump(list(trainset.keys()), f)\n",
    "with open(f'data/datasets/testset_{int(time.time())}.pk', 'wb') as f:\n",
    "    pickle.dump(list(testset.keys()), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label = '1674741184'\n",
    "with open(f'data/datasets/trainset_{dataset_label}.pk', 'rb') as f:\n",
    "    trainset_ids = pickle.load(f)\n",
    "    trainset = {key: value for key, value in dataset.items() if key in trainset_ids}\n",
    "with open(f'data/datasets/testset_{dataset_label}.pk', 'rb') as f:\n",
    "    testset_ids = pickle.load(f)\n",
    "    testset = {key: value for key, value in dataset.items() if key in testset_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_quadrant_to_keys = dict()\n",
    "\n",
    "for music_id in trainset.keys():\n",
    "    quad = music_id_to_quadrant[music_id]\n",
    "    if quad not in trainset_quadrant_to_keys:\n",
    "        trainset_quadrant_to_keys[quad] = list()\n",
    "    trainset_quadrant_to_keys[quad].append(music_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(spec, title=None, ylabel=\"freq_bin\", aspect=\"auto\", xmax=None, ymax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or \"Spectrogram (db)\")\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(spec), origin=\"lower\", aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    if ymax:\n",
    "        axs.set_ylim((0, ymax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram(dataset[1000][0], title=\"MelSpectrogram - torchaudio\", ylabel=\"mel freq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLSTMCNN(nn.Module):\n",
    "    def __init__(self, input_shape: Tuple[int, int], out_size: int = 2, cnn_channels: int = 64):\n",
    "        \"\"\"\n",
    "        :param input_shape: (X, Y). For a spectogram with 128 buckets and chunk size of 196, will be (128, 196)\n",
    "        \"\"\"\n",
    "        # call the parent constructor\n",
    "        super(AudioLSTMCNN, self).__init__()\n",
    "\n",
    "        self.conv11 = nn.Conv2d(in_channels=1, out_channels=cnn_channels, kernel_size=(3, 3), stride=(1, 1), padding=1)\n",
    "        self.relu11 = nn.ReLU()\n",
    "        self.conv12 = nn.Conv2d(in_channels=cnn_channels, out_channels=cnn_channels, kernel_size=(3, 3), stride=(1, 1),\n",
    "                                padding=1)\n",
    "        self.relu12 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv21 = nn.Conv2d(in_channels=cnn_channels, out_channels=cnn_channels, kernel_size=(3, 3), stride=(1, 1),\n",
    "                                padding=1)\n",
    "        self.relu21 = nn.ReLU()\n",
    "        self.conv22 = nn.Conv2d(in_channels=cnn_channels, out_channels=cnn_channels, kernel_size=(3, 3), stride=(1, 1),\n",
    "                                padding=1)\n",
    "        self.relu22 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=cnn_channels, out_channels=cnn_channels * 2, kernel_size=(3, 3),\n",
    "                               stride=(1, 1), padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        self.dropout3 = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=cnn_channels * 2, out_channels=cnn_channels * 4, kernel_size=(3, 3),\n",
    "                               stride=(1, 1), padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        self.dropout4 = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=cnn_channels * 4, out_channels=cnn_channels * 4, kernel_size=(3, 3),\n",
    "                               stride=(1, 1), padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.maxpool5 = nn.MaxPool2d(kernel_size=(3, 3), stride=(3, 3))\n",
    "        self.dropout5 = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.lstm6 = nn.LSTM(cnn_channels * 4, cnn_channels * 4, batch_first=True)\n",
    "        self.hidden = (torch.zeros(1, 1, cnn_channels * 4),\n",
    "                       torch.zeros(1, 1, cnn_channels * 4))\n",
    "        self.fc6 = nn.Linear(in_features=cnn_channels * 4, out_features=cnn_channels * 4)\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc7 = nn.Linear(in_features=cnn_channels * 4, out_features=cnn_channels * 4)\n",
    "        self.dropout7 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.fc8 = nn.Linear(in_features=cnn_channels * 4, out_features=2)\n",
    "        self.final = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((1, 1, x.shape[0], -1))\n",
    "\n",
    "        x = self.conv11(x)\n",
    "        x = self.relu11(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.relu12(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv21(x)\n",
    "        x = self.relu21(x)\n",
    "        x = self.conv22(x)\n",
    "        x = self.relu22(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.maxpool5(x)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x, self.hidden = self.lstm6(x, self.hidden)\n",
    "        x = self.fc6(x)\n",
    "        x = self.dropout6(x)\n",
    "\n",
    "        x = self.fc7(x)\n",
    "        x = self.dropout7(x)\n",
    "\n",
    "        x = self.fc8(x)\n",
    "\n",
    "        final_x = self.final(x.reshape((-1)))\n",
    "        # final_x = final_x*2 - 1\n",
    "\n",
    "        return final_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "### Load model\n",
    "model_name = f'saved_models/AudioLSTMCNN_1674751233_014772344088200705.pt'\n",
    "model_c = AudioLSTMCNN(input_shape=(128, 196), out_size=2)\n",
    "model_c.load_state_dict(torch.load(model_name));\n",
    "model_c.eval().cuda()\n",
    "print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = AudioLSTMCNN(input_shape=(128, 196), out_size=2).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = torch.optim.Adam(model_c.parameters(), lr=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 196])\n",
      "torch.Size([1, 1, 128, 196])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[10][0][0].shape)\n",
    "print(dataset[10][0][0].reshape((1, 1, MEL_SPECTROGRAM_BUCKETS, -1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0-   0 | loss: 0.10694  |  [   0(   0),    0(   0),    1(   1),    0(   0)]     lr: 0.005\n",
      " 0- 500 | loss: 0.15194  |  [   1(  89),    0( 137),  302( 141),  198( 134)]     lr: 0.005\n",
      " 0-1000 | loss: 0.12511  |  [   1( 142),    0( 195),  560( 328),  440( 336)]     lr: 0.005\n",
      " 0-1500 | loss: 0.09793  |  [ 206( 368),  130( 369),  646( 373),  519( 391)]     lr: 0.005\n",
      " 0-2000 | loss: 0.07536  |  [ 325( 503),  327( 564),  771( 478),  578( 456)]     lr: 0.005\n",
      " 1- 500 | loss: 0.00542  |  [  48(  32),  312( 330),  105(  94),   36(  45)]     lr: 0.005\n",
      " 1-1000 | loss: 0.00690  |  [ 300( 289),  395( 397),  146( 132),  160( 183)]     lr: 0.005\n",
      " 1-1500 | loss: 0.00698  |  [ 466( 449),  441( 443),  295( 291),  299( 318)]     lr: 0.005\n",
      " 1-2000 | loss: 0.00856  |  [ 536( 513),  521( 506),  464( 506),  480( 476)]     lr: 0.005\n",
      " 2- 500 | loss: 0.00443  |  [ 101( 102),  156( 169),   84(  59),  160( 171)]     lr: 0.005\n",
      " 2-1000 | loss: 0.00958  |  [ 310( 309),  234( 278),  221( 176),  236( 238)]     lr: 0.005\n",
      " 2-1500 | loss: 0.00776  |  [ 371( 342),  313( 351),  429( 391),  388( 417)]     lr: 0.005\n",
      " 2-2000 | loss: 0.00724  |  [ 435( 400),  391( 422),  583( 556),  592( 623)]     lr: 0.005\n",
      " 3- 500 | loss: 0.00681  |  [ 326( 315),  102( 131),   17(   0),   56(  55)]     lr: 0.005\n",
      " 3-1000 | loss: 0.00639  |  [ 517( 504),  190( 226),  112(  65),  182( 206)]     lr: 0.005\n",
      " 3-1500 | loss: 0.00577  |  [ 574( 537),  316( 379),  285( 230),  326( 355)]     lr: 0.005\n",
      " 3-2000 | loss: 0.00554  |  [ 632( 575),  481( 537),  429( 393),  459( 496)]     lr: 0.005\n",
      " 4- 500 | loss: 0.01038  |  [ 199( 169),  114( 137),  120(  86),   68( 109)]     lr: 0.005\n",
      " 4-1000 | loss: 0.00715  |  [ 405( 369),  184( 196),  210( 180),  202( 256)]     lr: 0.005\n",
      " 4-1500 | loss: 0.00588  |  [ 416( 382),  303( 310),  424( 408),  358( 401)]     lr: 0.005\n",
      " 4-2000 | loss: 0.00602  |  [ 455( 408),  497( 520),  592( 556),  457( 517)]     lr: 0.005\n",
      " 5- 500 | loss: 0.00588  |  [ 152( 123),   70(  89),  128( 128),  151( 161)]     lr: 0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-372-1c3389c99bd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aimpathy\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\aimpathy\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "EPOCS = 50\n",
    "PRINT_MARK = 500\n",
    "BATCH_SIZE = 50\n",
    "STOP_LOSS = 0.0025\n",
    "MIN_LEARNING_RATE = 0.0005\n",
    "\n",
    "normalize = lambda x: (x + 1)/2\n",
    "denormalize = lambda x: x*2 - 1\n",
    "\n",
    "\n",
    "for epoc in range(EPOCS):\n",
    "    losses = list()\n",
    "    quadrants = list()\n",
    "    real_quadrants = list()\n",
    "    # model_c.hidden = (model_c.hidden[0].cuda(), model_c.hidden[1].cuda())\n",
    "    train_key_sample = [key for keys in [random.sample(trainset_quadrant_to_keys[i+1], BATCH_SIZE//4) for i in range(4)] for key in keys]\n",
    "    random.shuffle(train_key_sample)\n",
    "    train_sample = [datum for sample_key in train_key_sample for datum in trainset[sample_key]]\n",
    "    # train_sample = [datum for key, value in trainset.items() if key in train_key_sample for datum in value]\n",
    "    for batch_i, (X_train, (valence, arousal)) in enumerate(train_sample):\n",
    "        optimizer.zero_grad()\n",
    "        model_c.hidden = (torch.zeros(model_c.hidden[0].shape).cuda(),\n",
    "                          torch.zeros(model_c.hidden[0].shape).cuda())\n",
    "        \n",
    "        y_train = torch.Tensor((valence, arousal)).cuda()\n",
    "        # Apply the model\n",
    "        y_pred = model_c(X_train.cuda())  # we don't flatten X-train here\n",
    "        loss = criterion(y_pred, y_train)\n",
    "\n",
    "        # Update parameters\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        real_quadrants.append(get_quadrant(y_train[0].item(), y_train[1].item()))\n",
    "        quadrants.append(get_quadrant(y_pred[0].item(), y_pred[1].item()))\n",
    "        \n",
    "        # Print interim results\n",
    "        if (batch_i > 0 or epoc == 0) and batch_i%PRINT_MARK == 0:\n",
    "            print(f'{epoc:2}-{batch_i:4} | loss: {np.mean(losses):.5f}  |  [{quadrants.count(1):4}({real_quadrants.count(1):4}), {quadrants.count(2):4}({real_quadrants.count(2):4}), {quadrants.count(3):4}({real_quadrants.count(3):4}), {quadrants.count(4):4}({real_quadrants.count(4):4})]     lr: {optimizer.param_groups[0][\"lr\"]}')\n",
    "                  # {y_train[0].item():.4f}, {y_train[0].item():.4f} | {y_pred[0].item():.4f},{y_pred[0].item():.4f}')\n",
    "                  # [{abs(y_train[0] - y_pred[0]):.5f}, {abs(y_train[1] - y_pred[1]):.5f}]\n",
    "    \n",
    "    scheduler.step(np.mean(losses))\n",
    "\n",
    "    if np.mean(losses) < STOP_LOSS or optimizer.param_groups[0][\"lr\"] < MIN_LEARNING_RATE:\n",
    "        break\n",
    "        \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_model_name = f'saved_models/AudioLSTMCNN_{int(time.time())}_{str(np.mean(losses))[2:]}.pt'\n",
    "torch.save(model_c.state_dict(), latest_model_name)\n",
    "print(f\"Saved model {latest_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 6928/6928 [00:30<00:00, 225.35it/s]\n"
     ]
    }
   ],
   "source": [
    "losses = list()\n",
    "quadrants = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_c.hidden = (model_c.hidden[0].cuda(), model_c.hidden[1].cuda())\n",
    "    test_sample = [datum for key, value in testset.items() for datum in value]\n",
    "    for batch_i, (X_test, (valence, arousal)) in tqdm(enumerate(test_sample), total=len(test_sample)):\n",
    "        y_test = torch.Tensor((valence, arousal)).cuda()\n",
    "        # Apply the model\n",
    "#         model_c.hidden = (torch.zeros(model_c.hidden[0].shape).cuda(),\n",
    "#                           torch.zeros(model_c.hidden[0].shape).cuda())\n",
    "        \n",
    "        y_val = model_c(X_test.cuda())\n",
    "        loss = criterion(y_val, y_test)\n",
    "        losses.append(loss.cpu())\n",
    "        quadrants.append((get_quadrant(y_val[0].item(), y_val[1].item()), get_quadrant(y_test[0].item(), y_test[1].item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.05365, median: 0.03952\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean: {np.mean(losses):.5f}, median: {np.median(losses):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0]\n",
      " [   0    0    1    0]\n",
      " [4584  307  772  771]\n",
      " [ 353   20   52   68]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.94      0.12      0.21      6434\n",
      "           4       0.08      0.14      0.10       493\n",
      "\n",
      "    accuracy                           0.12      6928\n",
      "   macro avg       0.25      0.06      0.08      6928\n",
      "weighted avg       0.87      0.12      0.20      6928\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amity\\Anaconda3\\envs\\aimpathy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amity\\Anaconda3\\envs\\aimpathy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\amity\\Anaconda3\\envs\\aimpathy\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix([a[0] for a in quadrants], [a[1] for a in quadrants]))\n",
    "print(\"\\n\")\n",
    "print(classification_report([a[0] for a in quadrants], [a[1] for a in quadrants]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect quadrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 519\n",
    "test_location = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model_c(testset[test_index][test_location][0].cuda()).cpu()\n",
    "print(pred)\n",
    "print([denormalize(a.item()) for a in pred])\n",
    "print(testset[test_index][test_location][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(key, value[0][1]) for key, value in testset.items() if value and value[0][1] < (0.5, 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random results classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random vs. testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 7064/7064 [00:01<00:00, 4382.98it/s]\n"
     ]
    }
   ],
   "source": [
    "losses = list()\n",
    "\n",
    "test_sample = [datum for key, value in testset.items() for datum in value]\n",
    "for batch_i, (X_test, (valence, arousal)) in tqdm(enumerate(test_sample), total=len(test_sample)):\n",
    "    y_test = torch.Tensor((valence, arousal)).cuda()\n",
    "    y_val = torch.Tensor((random.random(), random.random())).cuda()\n",
    "    loss = criterion(y_val, y_test)\n",
    "    losses.append(loss.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.13151, median: 0.10569\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean: {np.mean(losses):.5f}, median: {np.median(losses):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random vs. Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 7064/7064 [00:01<00:00, 4124.96it/s]\n"
     ]
    }
   ],
   "source": [
    "losses = list()\n",
    "\n",
    "test_sample = [datum for key, value in testset.items() for datum in value]\n",
    "for batch_i, (X_test, (valence, arousal)) in tqdm(enumerate(test_sample), total=len(test_sample)):\n",
    "    y_test = torch.Tensor((random.random(), random.random())).cuda()\n",
    "    y_val = torch.Tensor((random.random(), random.random())).cuda()\n",
    "    loss = criterion(y_val, y_test)\n",
    "    losses.append(loss.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.16612, median: 0.12879\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean: {np.mean(losses):.5f}, median: {np.median(losses):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = torch.Tensor((1, 1)).cuda()\n",
    "y_val = torch.Tensor((-1, -1)).cuda()\n",
    "loss = criterion(y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimpathy",
   "language": "python",
   "name": "aimpathy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
